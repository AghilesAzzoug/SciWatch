<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Article List</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css">
    <style>
        /* Reset some default styles */
        body {
            margin: 0;
            padding: 0;
            line-height: 1.5;
            font-family: Arial, sans-serif;
            color: #333333;
            background-color: #f1f1f1;
        }

        /* Styles for document separator */
        .separator {
            border-bottom: 1px solid #CCCCCC;
            margin: 20px 0;
        }

        /* Custom beautiful styles */
        .card {
            background-color: #ffffff;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        .card-title {
            color: #333333;
            font-weight: bold;
            font-size: 24px;
            margin-bottom: 10px;
        }

        .card-text {
            color: #555555;
            font-size: 16px;
            margin-bottom: 20px;
            overflow: hidden;
            text-overflow: ellipsis;
            display: -webkit-box;
            -webkit-line-clamp: 2;
            /* Number of lines to show */
            -webkit-box-orient: vertical;
        }

        .card-link {
            color: #0078d4;
            text-decoration: none;
        }

        .separator {
            border-color: #c8c8c8;
        }

        /* Media queries for responsiveness */
        @media (max-width: 576px) {
            .card-title {
                font-size: 20px;
                margin-bottom: 8px;
            }

            .card-text {
                font-size: 14px;
                margin-bottom: 16px;
            }
        }

        /* Style for the toggle button */
        .read-more-button {
            background: none;
            border: none;
            color: #0078d4;
            cursor: pointer;
            padding: 0;
            margin-top: 10px;
            font-size: 16px;
            font-weight: bold;
        }

        /* Style for the toggle button icon */
        .read-more-icon {
            display: inline-block;
            margin-right: 5px;
            transition: transform 0.2s;
        }

        /* Rotate the icon when the content is expanded */
        .read-more-toggle:checked + .card .read-more-icon {
            transform: rotate(180deg);
        }

        /* Hide content by default */
        .card .card-content {
            display: none;
        }

        /* Display content when toggle is checked */
        .read-more-toggle:checked + .card .card-content {
            display: block;
        }
    </style>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script>
        $(document).ready(function() {
            $('.read-more-button').click(function() {
                var card = $(this).closest('.card');
                var icon = card.find('.read-more-icon');
                var isChecked = card.find('.read-more-toggle').is(':checked');
                card.find('.read-more-toggle').prop('checked', !isChecked);
                if (!isChecked) {
                    icon.css('transform', 'rotate(180deg)');
                    card.find('.card-content').show();
                } else {
                    icon.css('transform', 'rotate(0deg)');
                    card.find('.card-content').hide();
                }
            });
        });
    </script>
</head>

<body>
    <div class="container">
        <div class="row">
            <div class="col-md-8 offset-md-2">
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent</h2>
                        <p class="card-summary">LLM-Grounder is a zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline that utilizes an LLM to decompose complex natural language queries and a visual grounding tool to identify objects in a 3D scene, achieving state-of-the-art zero-shot grounding accuracy without requiring labeled training data.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.12311v1" class="card-link">http://arxiv.org/pdf/2309.12311v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">3D visual grounding is a critical skill for household robots, enabling them
to navigate, manipulate objects, and answer questions based on their
environment. While existing approaches often rely on extensive labeled data or
exhibit limitations in handling complex language queries, we propose
LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model
(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to
decompose complex natural language queries into semantic constituents and
employs a visual grounding tool, such as OpenScene or LERF, to identify objects
in a 3D scene. The LLM then evaluates the spatial and commonsense relations
among the proposed objects to make a final grounding decision. Our method does
not require any labeled training data and can generalize to novel 3D scenes and
arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and
demonstrate state-of-the-art zero-shot grounding accuracy. Our findings
indicate that LLMs significantly improve the grounding capability, especially
for complex language queries, making LLM-Grounder an effective approach for 3D
vision-language tasks in robotics. Videos and interactive demos can be found on
the project website https://chat-with-nerf.github.io/ .</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models</h2>
                        <p class="card-summary">The text introduces LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs) with limited computation cost, achieving similar performance to fine-tuning with vanilla attention and demonstrating strong empirical results on various tasks.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.12307v1" class="card-link">http://arxiv.org/pdf/2309.12307v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">We present LongLoRA, an efficient fine-tuning approach that extends the
context sizes of pre-trained large language models (LLMs), with limited
computation cost. Typically, training LLMs with long context sizes is
computationally expensive, requiring extensive training hours and GPU
resources. For example, training on the context length of 8192 needs 16x
computational costs in self-attention layers as that of 2048. In this paper, we
speed up the context extension of LLMs in two aspects. On the one hand,
although dense global attention is needed during inference, fine-tuning the
model can be effectively and efficiently done by sparse local attention. The
proposed shift short attention effectively enables context extension, leading
to non-trivial computation saving with similar performance to fine-tuning with
vanilla attention. Particularly, it can be implemented with only two lines of
code in training, while being optional in inference. On the other hand, we
revisit the parameter-efficient fine-tuning regime for context expansion.
Notably, we find that LoRA for context extension works well under the premise
of trainable embedding and normalization. LongLoRA demonstrates strong
empirical results on various tasks on LLaMA2 models from 7B/13B to 70B.
LongLoRA adopts LLaMA2 7B from 4k context to 100k, or LLaMA2 70B to 32k on a
single 8x A100 machine. LongLoRA extends models' context while retaining their
original architectures, and is compatible with most existing techniques, like
FlashAttention-2. In addition, to make LongLoRA practical, we collect a
dataset, LongQA, for supervised fine-tuning. It contains more than 3k long
context question-answer pairs.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models</h2>
                        <p class="card-summary">This work proposes a generate-and-rerank approach to improve the output quality of large language models in generating natural language from logical forms, demonstrating its effectiveness through extensive experiments and comprehensive metrics.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.12294v1" class="card-link">http://arxiv.org/pdf/2309.12294v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">Large language models (LLMs) have demonstrated impressive capabilities in
natural language generation. However, their output quality can be inconsistent,
posing challenges for generating natural language from logical forms (LFs).
This task requires the generated outputs to embody the exact semantics of LFs,
without missing any LF semantics or creating any hallucinations. In this work,
we tackle this issue by proposing a novel generate-and-rerank approach. Our
approach involves initially generating a set of candidate outputs by prompting
an LLM and subsequently reranking them using a task-specific reranker model. In
addition, we curate a manually collected dataset to evaluate the alignment
between different ranking metrics and human judgements. The chosen ranking
metrics are utilized to enhance the training and evaluation of the reranker
model. By conducting extensive experiments on three diverse datasets, we
demonstrate that the candidates selected by our reranker outperform those
selected by baseline methods in terms of semantic consistency and fluency, as
measured by three comprehensive metrics. Our findings provide strong evidence
for the effectiveness of our approach in improving the quality of generated
outputs.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"</h2>
                        <p class="card-summary">Auto-regressive large language models (LLMs) exhibit a failure of generalization known as the Reversal Curse, where they are unable to correctly answer questions in the reverse direction of their training, indicating a basic failure of logical deduction and a lack of generalization of prevalent patterns in their training set.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.12288v1" class="card-link">http://arxiv.org/pdf/2309.12288v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">We expose a surprising failure of generalization in auto-regressive large
language models (LLMs). If a model is trained on a sentence of the form "A is
B", it will not automatically generalize to the reverse direction "B is A".
This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz
was the ninth Chancellor of Germany", it will not automatically be able to
answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the
likelihood of the correct answer ("Olaf Scholz") will not be higher than for a
random name. Thus, models exhibit a basic failure of logical deduction and do
not generalize a prevalent pattern in their training set (i.e. if "A is B''
occurs, "B is A" is more likely to occur). We provide evidence for the Reversal
Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah
Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to
correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Curse is
robust across model sizes and model families and is not alleviated by data
augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about
real-world celebrities, such as "Who is Tom Cruise's mother? [A: Mary Lee
Pfeiffer]" and the reverse "Who is Mary Lee Pfeiffer's son?". GPT-4 correctly
answers questions like the former 79% of the time, compared to 33% for the
latter. This shows a failure of logical deduction that we hypothesize is caused
by the Reversal Curse. Code is available at
https://github.com/lukasberglund/reversal_curse.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models</h2>
                        <p class="card-summary">The authors propose MetaMath, a fine-tuned language model specializing in mathematical reasoning, which outperforms existing open-source models on popular benchmarks and achieves state-of-the-art results, and they release the MetaMathQA dataset, MetaMath models, and training code for public use.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.12284v1" class="card-link">http://arxiv.org/pdf/2309.12284v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">Large language models (LLMs) have pushed the limits of natural language
understanding and exhibited excellent problem-solving ability. Despite the
great success, most existing open-source LLMs (\eg, LLaMA-2) are still far away
from satisfactory for solving mathematical problem due to the complex reasoning
procedures. To bridge this gap, we propose \emph{MetaMath}, a fine-tuned
language model that specializes in mathematical reasoning. Specifically, we
start by bootstrapping mathematical questions by rewriting the question from
multiple perspectives without extra knowledge, which results in a new dataset
called {MetaMathQA}. Then we fine-tune the LLaMA-2 models on MetaMathQA.
Experimental results on two popular benchmarks (\ie, GSM8K and MATH) for
mathematical reasoning demonstrate that MetaMath outperforms a suite of
open-source LLMs by a significant margin. Our MetaMath-7B model achieves
$66.4\%$ on GSM8K and $19.4\%$ on MATH, exceeding the state-of-the-art models
of the same size by $11.5\%$ and $8.7\%$. Particularly, {MetaMath-70B} achieves
an accuracy of $82.3\%$ on {GSM8K}, slightly better than {GPT-3.5-Turbo}. We
release the {MetaMathQA} dataset, the {MetaMath} models with different model
sizes and the training code for public use.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition</h2>
                        <p class="card-summary">This paper proposes a two-step approach using a large language model to improve Biomedical Named Entity Recognition (NER) by breaking down the task into entity span extraction and entity type determination, and incorporating external knowledge to enhance performance.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.12278v1" class="card-link">http://arxiv.org/pdf/2309.12278v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">Large language models (LLMs) have demonstrated dominating performance in many
NLP tasks, especially on generative tasks. However, they often fall short in
some information extraction tasks, particularly those requiring domain-specific
knowledge, such as Biomedical Named Entity Recognition (NER). In this paper,
inspired by Chain-of-thought, we leverage the LLM to solve the Biomedical NER
step-by-step: break down the NER task into entity span extraction and entity
type determination. Additionally, for entity type determination, we inject
entity knowledge to address the problem that LLM's lack of domain knowledge
when predicting entity category. Experimental results show a significant
improvement in our two-step BioNER approach compared to previous few-shot LLM
baseline. Additionally, the incorporation of external knowledge significantly
enhances entity category determination performance.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] LLMR: Real-time Prompting of Interactive Worlds using Large Language Models</h2>
                        <p class="card-summary">The Large Language Model for Mixed Reality (LLMR) is a framework that allows for the real-time creation and modification of interactive Mixed Reality experiences using text interaction and the Unity game engine, outperforming the standard GPT-4 by 4x in average error rate, demonstrating cross-platform interoperability and positive user experiences.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.12276v1" class="card-link">http://arxiv.org/pdf/2309.12276v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">We present Large Language Model for Mixed Reality (LLMR), a framework for the
real-time creation and modification of interactive Mixed Reality experiences
using LLMs. LLMR leverages novel strategies to tackle difficult cases where
ideal training data is scarce, or where the design goal requires the synthesis
of internal dynamics, intuitive analysis, or advanced interactivity. Our
framework relies on text interaction and the Unity game engine. By
incorporating techniques for scene understanding, task planning,
self-debugging, and memory management, LLMR outperforms the standard GPT-4 by
4x in average error rate. We demonstrate LLMR's cross-platform interoperability
with several example worlds, and evaluate it on a variety of creation and
modification tasks to show that it can produce and edit diverse objects, tools,
and scenes. Finally, we conducted a usability study (N=11) with a diverse set
that revealed participants had positive experiences with the system and would
use it again.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] On the Relationship between Skill Neurons and Robustness in Prompt Tuning</h2>
                        <p class="card-summary">This paper examines the robustness of Prompt Tuning, a parameter-efficient finetuning method for large language models, in relation to "skill neurons" that are highly predictive and selective for specific tasks, finding that prompts tuned for a specific task are transferable but not robust to adversarial data, with T5 showing higher robustness than RoBERTa, and that skill neurons exist in both models but are more predictive on adversarial data in T5.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.12263v1" class="card-link">http://arxiv.org/pdf/2309.12263v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">Prompt Tuning is a popular parameter-efficient finetuning method for
pre-trained large language models (PLMs). Recently, based on experiments with
RoBERTa, it has been suggested that Prompt Tuning activates specific neurons in
the transformer's feed-forward networks, that are highly predictive and
selective for the given task. In this paper, we study the robustness of Prompt
Tuning in relation to these "skill neurons", using RoBERTa and T5. We show that
prompts tuned for a specific task are transferable to tasks of the same type
but are not very robust to adversarial data, with higher robustness for T5 than
RoBERTa. At the same time, we replicate the existence of skill neurons in
RoBERTa and further show that skill neurons also seem to exist in T5.
Interestingly, the skill neurons of T5 determined on non-adversarial data are
also among the most predictive neurons on the adversarial data, which is not
the case for RoBERTa. We conclude that higher adversarial robustness may be
related to a model's ability to activate the relevant skill neurons on
adversarial data.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection</h2>
                        <p class="card-summary">The paper investigates the potential of large language models (LLMs) in detecting fake news, finding that while LLMs can provide multi-perspective rationales, they underperform compared to small language models (SLMs), suggesting that LLMs can serve as advisors to SLMs in fake news detection; the authors propose an adaptive rationale guidance network (ARG) that selectively acquires insights from LLMs for SLMs, and a rationale-free version (ARG-D) for cost-sensitive scenarios, both of which outperform baseline methods in detecting fake news.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.12247v1" class="card-link">http://arxiv.org/pdf/2309.12247v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">Detecting fake news requires both a delicate sense of diverse clues and a
profound understanding of the real-world background, which remains challenging
for detectors based on small language models (SLMs) due to their knowledge and
capability limitations. Recent advances in large language models (LLMs) have
shown remarkable performance in various tasks, but whether and how LLMs could
help with fake news detection remains underexplored. In this paper, we
investigate the potential of LLMs in fake news detection. First, we conduct an
empirical study and find that a sophisticated LLM such as GPT 3.5 could
generally expose fake news and provide desirable multi-perspective rationales
but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis
attributes such a gap to the LLM's inability to select and integrate rationales
properly to conclude. Based on these findings, we propose that current LLMs may
not substitute fine-tuned SLMs in fake news detection but can be a good advisor
for SLMs by providing multi-perspective instructive rationales. To instantiate
this proposal, we design an adaptive rationale guidance network for fake news
detection (ARG), in which SLMs selectively acquire insights on news analysis
from the LLMs' rationales. We further derive a rationale-free version of ARG by
distillation, namely ARG-D, which services cost-sensitive scenarios without
inquiring LLMs. Experiments on two real-world datasets demonstrate that ARG and
ARG-D outperform three types of baseline methods, including SLM-based,
LLM-based, and combinations of small and large language models.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events</h2>
                        <p class="card-summary">ChaCha is a chatbot designed to help children share personal events and emotions, using a combination of state machine and large language models, and an exploratory study found that children perceived ChaCha as a close friend and were willing to share their stories and emotions with it.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.12244v1" class="card-link">http://arxiv.org/pdf/2309.12244v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">Children typically learn to identify and express emotions through sharing
their stories and feelings with others, particularly their family. However, it
is challenging for parents or siblings to have emotional communication with
children since children are still developing their communication skills. We
present ChaCha, a chatbot that encourages and guides children to share personal
events and associated emotions. ChaCha combines a state machine and large
language models (LLMs) to keep the dialogue on track while carrying on
free-form conversations. Through an exploratory study with 20 children (aged
8-12), we examine how ChaCha prompts children to share personal events and
guides them to describe associated emotions. Participants perceived ChaCha as a
close friend and shared their stories on various topics, such as family trips
and personal achievements. Based on the quantitative and qualitative findings,
we discuss opportunities for leveraging LLMs to design child-friendly chatbots
to support children in sharing their emotions.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] Code Soliloquies for Accurate Calculations in Large Language Models</h2>
                        <p class="card-summary">This paper introduces a stateful prompt design that uses GPT-4 to generate synthetic student-teacher dialogues, addressing the limitations of GPT-4 in handling complex calculations and enhancing the quality and computational reliability of the conversations for calculation-intensive subjects.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.12161v1" class="card-link">http://arxiv.org/pdf/2309.12161v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">High-quality conversational datasets are integral to the successful
development of Intelligent Tutoring Systems (ITS) that employ a Large Language
Model (LLM) backend. These datasets, when used to fine-tune the LLM backend,
significantly enhance the quality of interactions between students and ITS. A
common strategy for developing these datasets involves generating synthetic
student-teacher dialogues using advanced GPT-4 models. However, challenges
arise when these dialogues demand complex calculations, common in subjects like
physics. Despite its advanced capabilities, GPT-4's performance falls short in
reliably handling even simple multiplication tasks, marking a significant
limitation in its utility for these subjects. To address these challenges, this
paper introduces an innovative stateful prompt design. Our approach generates a
mock conversation between a student and a tutorbot, both roles simulated by
GPT-4. Each student response triggers a soliloquy (an inner monologue) in the
GPT-tutorbot, which assesses whether its response would necessitate
calculations. If so, it proceeds to script the required code in Python and then
uses the resulting output to construct its response to the student. Our
approach notably enhances the quality of synthetic conversation datasets,
especially for subjects that are calculation-intensive. Our findings show that
our Higgs model -- a LLaMA finetuned with datasets generated through our novel
stateful prompt design -- proficiently utilizes Python for computations.
Consequently, finetuning with our datasets enriched with code soliloquies
enhances not just the accuracy but also the computational reliability of Higgs'
responses.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models</h2>
                        <p class="card-summary">Research on efficient fine-tuning strategies for low-resource languages like Tibetan is necessary, and this study explores three types of efficient fine-tuning experiments on a Tibetan dataset, demonstrating significant improvements and providing valuable insights for advancing Tibetan language applications.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.12109v1" class="card-link">http://arxiv.org/pdf/2309.12109v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">In this era of large language models (LLMs), the traditional training of
models has become increasingly unimaginable for regular users and institutions.
The exploration of efficient fine-tuning for high-resource languages on these
models is an undeniable trend that is gradually gaining popularity. However,
there has been very little exploration for various low-resource languages, such
as Tibetan. Research in Tibetan NLP is inherently scarce and limited. While
there is currently no existing large language model for Tibetan due to its
low-resource nature, that day will undoubtedly arrive. Therefore, research on
efficient fine-tuning for low-resource language models like Tibetan is highly
necessary. Our research can serve as a reference to fill this crucial gap.
Efficient fine-tuning strategies for pre-trained language models (PLMs) in
Tibetan have seen minimal exploration. We conducted three types of efficient
fine-tuning experiments on the publicly available TNCC-title dataset:
"prompt-tuning," "Adapter lightweight fine-tuning," and "prompt-tuning +
Adapter fine-tuning." The experimental results demonstrate significant
improvements using these methods, providing valuable insights for advancing
Tibetan language applications in the context of pre-trained models.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] SemEval-2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts</h2>
                        <p class="card-summary">This text describes SemEval-2022 Task 7, a shared task on rating the plausibility of clarifications in instructional texts, with 21 participants and the best system achieving an accuracy of 68.9%, and also summarizes the results and findings from 8 teams and their system descriptions, showing that the top participating team's predictions can identify contexts with multiple plausible clarifications with an accuracy of 75.2%.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.12102v1" class="card-link">http://arxiv.org/pdf/2309.12102v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">We describe SemEval-2022 Task 7, a shared task on rating the plausibility of
clarifications in instructional texts. The dataset for this task consists of
manually clarified how-to guides for which we generated alternative
clarifications and collected human plausibility judgements. The task of
participating systems was to automatically determine the plausibility of a
clarification in the respective context. In total, 21 participants took part in
this task, with the best system achieving an accuracy of 68.9%. This report
summarizes the results and findings from 8 teams and their system descriptions.
Finally, we show in an additional evaluation that predictions by the top
participating team make it possible to identify contexts with multiple
plausible clarifications with an accuracy of 75.2%.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models</h2>
                        <p class="card-summary">This study evaluates the performance and efficiency of Prompt Tuning compared to baseline methods for multi-label text classification, specifically in classifying companies for an investment firm's industry taxonomy, and finds that replacing the language head of Pretrained Language Models with a classification head improves performance and reduces computational costs during inference.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.12075v1" class="card-link">http://arxiv.org/pdf/2309.12075v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">Prompt Tuning is emerging as a scalable and cost-effective method to
fine-tune Pretrained Language Models (PLMs). This study benchmarks the
performance and computational efficiency of Prompt Tuning and baseline methods
on a multi-label text classification task. This is applied to the use case of
classifying companies into an investment firm's proprietary industry taxonomy,
supporting their thematic investment strategy. Text-to-text classification with
PLMs is frequently reported to outperform classification with a classification
head, but has several limitations when applied to a multi-label classification
problem where each label consists of multiple tokens: (a) Generated labels may
not match any label in the industry taxonomy; (b) During fine-tuning, multiple
labels must be provided in an arbitrary order; (c) The model provides a binary
decision for each label, rather than an appropriate confidence score.
Limitation (a) is addressed by applying constrained decoding using Trie Search,
which slightly improves classification performance. All limitations (a), (b),
and (c) are addressed by replacing the PLM's language head with a
classification head. This improves performance significantly, while also
reducing computational costs during inference. The results indicate the
continuing need to adapt state-of-the-art methods to domain-specific tasks,
even in the era of PLMs with strong generalization abilities.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] AceGPT, Localizing Large Language Models in Arabic</h2>
                        <p class="card-summary">This paper discusses the need for a localized Large Language Model (LLM) for Arabic, outlining a solution that includes pre-training, fine-tuning, and reinforcement learning to create culturally aware and value-aligned Arabic LLMs called AceGPT, which outperforms ChatGPT in various benchmarks.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.12053v1" class="card-link">http://arxiv.org/pdf/2309.12053v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">This paper explores the imperative need and methodology for developing a
localized Large Language Model (LLM) tailored for Arabic, a language with
unique cultural characteristics that are not adequately addressed by current
mainstream models like ChatGPT. Key concerns additionally arise when
considering cultural sensitivity and local values. To this end, the paper
outlines a packaged solution, including further pre-training with Arabic texts,
supervised fine-tuning (SFT) using native Arabic instructions and GPT-4
responses in Arabic, and reinforcement learning with AI feedback (RLAIF) using
a reward model that is sensitive to local culture and values. The objective is
to train culturally aware and value-aligned Arabic LLMs that can serve the
diverse application-specific needs of Arabic-speaking communities.
  Extensive evaluations demonstrated that the resulting LLM called
`\textbf{AceGPT}' is the SOTA open Arabic LLM in various benchmarks, including
instruction-following benchmark (i.e., Arabic Vicuna-80 and Arabic AlpacaEval),
knowledge benchmark (i.e., Arabic MMLU and EXAMs), as well as the
newly-proposed Arabic cultural \& value alignment benchmark. Notably, AceGPT
outperforms ChatGPT in the popular Vicuna-80 benchmark when evaluated with
GPT-4, despite the benchmark's limited scale. % Natural Language Understanding
(NLU) benchmark (i.e., ALUE)
  Codes, data, and models are in https://github.com/FreedomIntelligence/AceGPT.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset</h2>
                        <p class="card-summary">This paper introduces LMSYS-Chat-1M, a large-scale dataset of one million real-world conversations with 25 state-of-the-art language models, highlighting its diversity and usefulness for developing content moderation models, safety benchmarks, instruction-following models, and challenging benchmark questions.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.11998v1" class="card-link">http://arxiv.org/pdf/2309.11998v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">Studying how people interact with large language models (LLMs) in real-world
scenarios is increasingly important due to their widespread use in various
applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset
containing one million real-world conversations with 25 state-of-the-art LLMs.
This dataset is collected from 210K unique IP addresses in the wild on our
Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's
content, including its curation process, basic statistics, and topic
distribution, highlighting its diversity, originality, and scale. We
demonstrate its versatility through four use cases: developing content
moderation models that perform similarly to GPT-4, building a safety benchmark,
training instruction-following models that perform similarly to Vicuna, and
creating challenging benchmark questions. We believe that this dataset will
serve as a valuable resource for understanding and advancing LLM capabilities.
The dataset is publicly available at
\url{https://huggingface.co/datasets/lmsys/lmsys-chat-1m}.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts</h2>
                        <p class="card-summary">The paper proposes a novel zero-shot text-to-speech (TTS) model that utilizes multi-scale acoustic prompts to clone an unseen speaker's voice with their personal speaking style, achieving better performance in terms of naturalness and speaker similarity compared to baselines.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.11977v1" class="card-link">http://arxiv.org/pdf/2309.11977v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's
voice without adaptation parameters. By quantizing speech waveform into
discrete acoustic tokens and modeling these tokens with the language model,
recent language model-based TTS models show zero-shot speaker adaptation
capabilities with only a 3-second acoustic prompt of an unseen speaker.
However, they are limited by the length of the acoustic prompt, which makes it
difficult to clone personal speaking style. In this paper, we propose a novel
zero-shot TTS model with the multi-scale acoustic prompts based on a neural
codec language model VALL-E. A speaker-aware text encoder is proposed to learn
the personal speaking style at the phoneme-level from the style prompt
consisting of multiple sentences. Following that, a VALL-E based acoustic
decoder is utilized to model the timbre from the timbre prompt at the
frame-level and generate speech. The experimental results show that our
proposed method outperforms baselines in terms of naturalness and speaker
similarity, and can achieve better performance by scaling out to a longer style
prompt.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework</h2>
                        <p class="card-summary">The study proposes a new approach called InstructERC that reformulates emotion recognition in dialogue using Large Language Models (LLMs), achieving state-of-the-art performance on multiple datasets by integrating dialogue supervision information and introducing additional emotion alignment tasks.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.11911v1" class="card-link">http://arxiv.org/pdf/2309.11911v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">The development of emotion recognition in dialogue (ERC) has been
consistently hindered by the complexity of pipeline designs, leading to ERC
models that often overfit to specific datasets and dialogue patterns. In this
study, we propose a novel approach, namely
  InstructERC, to reformulates the ERC task from a discriminative framework to
a generative framework based on Large Language Models (LLMs) . InstructERC has
two significant contributions: Firstly, InstructERC introduces a simple yet
effective retrieval template module, which helps the model explicitly integrate
multi-granularity dialogue supervision information by concatenating the
historical dialog content, label statement, and emotional domain demonstrations
with high semantic similarity. Furthermore, we introduce two additional emotion
alignment tasks, namely speaker identification and emotion prediction tasks, to
implicitly model the dialogue role relationships and future emotional
tendencies in conversations. Our LLM-based plug-and-play plugin framework
significantly outperforms all previous models and achieves comprehensive SOTA
on three commonly used ERC datasets. Extensive analysis of parameter-efficient
and data-scaling experiments provide empirical guidance for applying
InstructERC in practical scenarios. Our code will be released after blind
review.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] Knowledge Sanitization of Large Language Models</h2>
                        <p class="card-summary">The text discusses a knowledge sanitization approach to address privacy concerns with large language models, which involves fine-tuning the models to generate harmless responses and minimize knowledge leakage while maintaining overall performance, thereby enhancing defense against extraction attacks and reducing harmful content.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.11852v1" class="card-link">http://arxiv.org/pdf/2309.11852v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">We explore a knowledge sanitization approach to mitigate the privacy concerns
associated with large language models (LLMs). LLMs trained on a large corpus of
Web data can memorize and potentially reveal sensitive or confidential
information, raising critical security concerns. Our technique fine-tunes these
models, prompting them to generate harmless responses such as ``I don't know''
when queried about specific information. Experimental results in a closed-book
question-answering task show that our straightforward method not only minimizes
particular knowledge leakage but also preserves the overall performance of LLM.
These two advantages strengthen the defense against extraction attacks and
reduces the emission of harmful content such as hallucinations.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues</h2>
                        <p class="card-summary">This paper investigates the use of large language models for generating responses in information-seeking dialogues, using the MultiDoc2Dial corpus for evaluation, and finds that while the language models may include additional information not present in the relevant documents, they are rated higher than the shared task winning system and human responses.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.11838v1" class="card-link">http://arxiv.org/pdf/2309.11838v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">In this paper, we investigate the use of large language models (LLMs) like
ChatGPT for document-grounded response generation in the context of
information-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus
of task-oriented dialogues in four social service domains previously used in
the DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded
in multiple documents providing relevant information. We generate dialogue
completion responses by prompting a ChatGPT model, using two methods:
Chat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT
model pretraining while LlamaIndex also extracts relevant information from
documents. Observing that document-grounded response generation via LLMs cannot
be adequately assessed by automatic evaluation metrics as they are
significantly more verbose, we perform a human evaluation where annotators rate
the output of the shared task winning system, the two Chat-GPT variants
outputs, and human responses. While both ChatGPT variants are more likely to
include information not present in the relevant segments, possibly including a
presence of hallucinations, they are rated higher than both the shared task
winning system and human responses.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] A Chinese Prompt Attack Dataset for LLMs with Evil Content</h2>
                        <p class="card-summary">Researchers have developed a Chinese Prompt Attack Dataset (CPAD) to evaluate the abilities of defending prompt attacks on Large Language Models (LLMs), finding that the prompts are significantly harmful to LLMs with a 70% attack success rate, and they plan to release CPAD to encourage further studies on prompt attack and defense.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.11830v1" class="card-link">http://arxiv.org/pdf/2309.11830v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">Large Language Models (LLMs) present significant priority in text
understanding and generation. However, LLMs suffer from the risk of generating
harmful contents especially while being employed to applications. There are
several black-box attack methods, such as Prompt Attack, which can change the
behaviour of LLMs and induce LLMs to generate unexpected answers with harmful
contents. Researchers are interested in Prompt Attack and Defense with LLMs,
while there is no publicly available dataset to evaluate the abilities of
defending prompt attack. In this paper, we introduce a Chinese Prompt Attack
Dataset for LLMs, called CPAD. Our prompts aim to induce LLMs to generate
unexpected outputs with several carefully designed prompt attack approaches and
widely concerned attacking contents. Different from previous datasets involving
safety estimation, We construct the prompts considering three dimensions:
contents, attacking methods and goals, thus the responses can be easily
evaluated and analysed. We run several well-known Chinese LLMs on our dataset,
and the results show that our prompts are significantly harmful to LLMs, with
around 70% attack success rate. We will release CPAD to encourage further
studies on prompt attack and defense.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination</h2>
                        <p class="card-summary">This study proposes a computational bionic memory mechanism to personalize large language models (LLMs) without the need for full retraining, demonstrating its effectiveness and superiority through extensive experimental results and releasing a new conversation dataset and implementation code.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.11696v1" class="card-link">http://arxiv.org/pdf/2309.11696v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable
proficiency in comprehending and generating natural language. However, their
unpersonalized generation paradigm may result in suboptimal user-specific
outcomes. Typically, users converse differently based on their knowledge and
preferences. This necessitates the task of enhancing user-oriented LLM which
remains unexplored. While one can fully train an LLM for this objective, the
resource consumption is unaffordable. Prior research has explored memory-based
methods to store and retrieve knowledge to enhance generation without
retraining for new queries. However, we contend that a mere memory module is
inadequate to comprehend a user's preference, and fully training an LLM can be
excessively costly. In this study, we propose a novel computational bionic
memory mechanism, equipped with a parameter-efficient fine-tuning schema, to
personalize LLMs. Our extensive experimental results demonstrate the
effectiveness and superiority of the proposed approach. To encourage further
research into this area, we are releasing a new conversation dataset generated
entirely by LLM based on an open-source medical corpus, as well as our
implementation code.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models</h2>
                        <p class="card-summary">The study proposes a novel fine-tuning approach for Large Language Models (LLMs) specifically designed for translation tasks, achieving significant improvements in translation performance compared to previous methods and even outperforming models with larger parameter sizes, establishing a new training paradigm in machine translation.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.11674v1" class="card-link">http://arxiv.org/pdf/2309.11674v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">Generative Large Language Models (LLMs) have achieved remarkable advancements
in various NLP tasks. However, these advances have not been reflected in the
translation task, especially those with moderate model sizes (i.e., 7B or 13B
parameters), which still lag behind conventional supervised encoder-decoder
translation models. Previous studies have attempted to improve the translation
capabilities of these moderate LLMs, but their gains have been limited. In this
study, we propose a novel fine-tuning approach for LLMs that is specifically
designed for the translation task, eliminating the need for the abundant
parallel data that traditional translation models usually depend on. Our
approach consists of two fine-tuning stages: initial fine-tuning on monolingual
data followed by subsequent fine-tuning on a small set of high-quality parallel
data. We introduce the LLM developed through this strategy as Advanced Language
Model-based trAnslator (ALMA). Based on LLaMA-2 as our underlying model, our
results show that the model can achieve an average improvement of more than 12
BLEU and 12 COMET over its zero-shot performance across 10 translation
directions from the WMT'21 (2 directions) and WMT'22 (8 directions) test
datasets. The performance is significantly better than all prior work and even
superior to the NLLB-54B model and GPT-3.5-text-davinci-003, with only 7B or
13B parameters. This method establishes the foundation for a novel training
paradigm in machine translation.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] Generative AI in Mafia-like Game Simulation</h2>
                        <p class="card-summary">This research explores the potential of Generative AI models, specifically GPT-4, in role-playing simulations like Spyfall, highlighting its improved adaptability and human-like responses, but also noting limitations in bluffing and predicting opponent moves, suggesting further development is needed.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.11672v1" class="card-link">http://arxiv.org/pdf/2309.11672v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">In this research, we explore the efficacy and potential of Generative AI
models, specifically focusing on their application in role-playing simulations
exemplified through Spyfall, a renowned mafia-style game. By leveraging GPT-4's
advanced capabilities, the study aimed to showcase the model's potential in
understanding, decision-making, and interaction during game scenarios.
Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo,
demonstrated GPT-4's enhanced adaptability to the game environment, with
significant improvements in posing relevant questions and forming human-like
responses. However, challenges such as the model;s limitations in bluffing and
predicting opponent moves emerged. Reflections on game development, financial
constraints, and non-verbal limitations of the study were also discussed. The
findings suggest that while GPT-4 exhibits promising advancements over earlier
models, there remains potential for further development, especially in
instilling more human-like attributes in AI.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] Towards Effective Disambiguation for Machine Translation with Large Language Models</h2>
                        <p class="card-summary">This paper explores the use of large language models (LLMs) to improve machine translation by addressing the challenge of resolving semantic ambiguity, specifically focusing on translating ambiguous sentences containing polysemous words and rare word senses, and proposes two methods to enhance disambiguation through in-context learning and fine-tuning on curated ambiguous datasets, achieving comparable or better performance than state-of-the-art systems in most language directions.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.11668v1" class="card-link">http://arxiv.org/pdf/2309.11668v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">Resolving semantic ambiguity has long been recognised as a central challenge
in the field of machine translation. Recent work on benchmarking translation
performance on ambiguous sentences has exposed the limitations of conventional
Neural Machine Translation (NMT) systems, which fail to capture many of these
cases. Large language models (LLMs) have emerged as a promising alternative,
demonstrating comparable performance to traditional NMT models while
introducing new paradigms for controlling the target outputs. In this paper, we
study the capabilities of LLMs to translate ambiguous sentences containing
polysemous words and rare word senses. We also propose two ways to improve the
handling of such ambiguity through in-context learning and fine-tuning on
carefully curated ambiguous datasets. Experiments show that our methods can
match or outperform state-of-the-art systems such as DeepL and NLLB in four out
of five language directions. Our research provides valuable insights into
effectively adapting LLMs for disambiguation during machine translation.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge</h2>
                        <p class="card-summary">Text-conditioned image generation models produce unsafe content due to their reliance on large datasets scraped from the web, and this study analyzes potential adversarial inputs to highlight the fragility of input filters and safety issues in current generative image models.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.11575v1" class="card-link">http://arxiv.org/pdf/2309.11575v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">Text-conditioned image generation models have recently achieved astonishing
image quality and alignment results. Consequently, they are employed in a
fast-growing number of applications. Since they are highly data-driven, relying
on billion-sized datasets randomly scraped from the web, they also produce
unsafe content. As a contribution to the Adversarial Nibbler challenge, we
distill a large set of over 1,000 potential adversarial inputs from existing
safety benchmarks. Our analysis of the gathered prompts and corresponding
images demonstrates the fragility of input filters and provides further
insights into systematic safety issues in current generative image models.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] The copyright issues around generative AI aren’t going away anytime soon</h2>
                        <p class="card-summary">Generative AI startups, including OpenAI and Runway, are facing copyright issues as authors and artists allege that their works were used to train AI models without their consent, leading to lawsuits and debates over fair use and copyright protection for AI-generated works.</p>
                        <p class="card-text">
                            <a href="https://techcrunch.com/2023/09/21/the-copyright-issues-around-generative-ai-arent-going-away-anytime-soon/" class="card-link">https://techcrunch.com/2023/09/21/the-copyright-issues-around-generative-ai-arent-going-away-anytime-soon/</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">
Generative AI has brought a host of copyright issues to the fore. Just this week, authors including George R.R. Martin, led by the Authors Guild, filed a lawsuit against OpenAI, alleging that the startup’s viral AI-powered chatbot, ChatGPT, was trained on their work without their knowledge or consent.
And it’s not just OpenAI that’s having to contend with this. Onstage at Disrupt 2023, Anastasis Germanidis, one of the co-founders of Runway, a company developing generative AI tools for video, said that his company is “still exploring” the right approach to training AI models on artists’ and creators’ works.
“We’re working closely with artists to figure out what the best approaches are to address this,” Germanidis said. “We’re exploring various data partnerships to be able to further grow … and build the next generation of models.”
Runway, like many generative AI startups, doesn’t disclose exactly where it sources the data it uses to train its models, including Gen-2, which generates videos from text prompts. In an academic paper detailing Gen-2’s architecture, Runway says only that the model was trained on an internal data set of 240 million images and 6.4 million video clips.
It could be that some of that data’s copyrighted. If it is, it might land Runway in hot water down the road.
Over the past year or so, artists have filed suit against Stability AI, Midjourney and DeviantArt, arguing that models released by the companies infringe on their copyrights by training on the artists’ works and generating outputs in their styles. Separately, Getty Images has sued Stability AI for allegedly copying and processing millions of images and associated metadata owned by Getty in the U.K.
Some companies developing generative AI tools argue that they’re protected by fair use doctrine, at least in the U.S. But it’s a matter that’s unlikely to be settled anytime soon.
To shield themselves from future legal challenges, a handful of generative AI vendors, including Stability AI, have introduced ways for artists to opt out of model training. (OpenAI just yesterday created a channel for artists to inform the company it can’t use their artwork for model training going forward.) Others have started communal funds to share some of the revenue generated by generative models with the artists whose data was used to train those models.
Runway doesn’t provide a way to opt out of training or a contributor fund. But Germanidis hinted that the company’s considering these.
“I think, for us, coming from a creative background has been key to how we’ve built this company and how we’re figuring out how to move this technology forward,” Germanidis said. “[We want] artists to feel that these products and tools work for them.”
What about the other side of the copyright debate, though — copyrighting AI-generated works? It’s an open question whether AI-generated works can be copyrighted. The U.S. Copyright Office only recently began soliciting comments on issues around generative AI and IP, and court decisions haven’t provided much clarity.
Germanidis asserted that Runway-generated content can be copyrighted, however. He stopped short of promising a policy like that recently adopted by Microsoft, which will pay any copyright-related legal damages for customers using Microsoft’s AI services. But Runway will defend customers if need be, Germanidis said. 
“We’re going to adapt to whatever changes in regulation [we need to,] but artists should feel confident using the platform,” he said. “We stand behind the content they create, and it belongs to them.”
</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] How generative AI is accelerating disinformation</h2>
                        <p class="card-summary">The rise of generative AI tools is making it easier and cheaper to generate and distribute disinformation on a massive scale, posing a threat to democracy and the spread of objective truth, with potential solutions including safeguards, watermarking, and economic incentives for generative AI companies to prioritize reliability.</p>
                        <p class="card-text">
                            <a href="https://techcrunch.com/2023/09/21/how-generative-ai-is-accelerating-disinformation/" class="card-link">https://techcrunch.com/2023/09/21/how-generative-ai-is-accelerating-disinformation/</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">
People are more aware of disinformation than they used to be. According to one recent poll, nine out of 10 American adults fact-check their news, and 96% want to limit the spread of false information.
But it’s becoming tougher — not easier — to stem the firehose of disinformation with the advent of generative AI tools.
That was the high-level takeaway from the disinformation and AI panel on the AI Stage at TechCrunch Disrupt 2023, which featured Sarah Brandt, the EVP of partnerships at NewsGuard, and Andy Parsons, the senior director of the Content Authenticity Initiative (CAI) at Adobe. The panelists spoke about the threat of AI-generated disinformation and potential solutions as an election year looms.
Parsons framed the stakes in fairly stark terms:
Without a core foundation and objective truth that we can share, frankly — without exaggeration — democracy is at stake. Being able to have objective conversations with other humans about shared truth is at stake.
Both Brandt and Parsons acknowledged that web-borne disinformation, AI-assisted or no, is hardly a new phenomenon. Parsons referred to the 2019 viral clip of former House Speaker Nancy Pelosi (D-CA), which used crude editing to make it appear as though Pelosi was speaking in a slurred, awkward way.
But Brandt also noted that — thanks to AI, particularly generative AI — it’s becoming a lot cheaper and simpler to generate and distribute disinformation on a massive scale.
She cited statistics from her work at NewsGuard, which develops a rating system for news and information websites and provides services such as misinformation tracking and brand safety for advertisers. In May, NewsGuard identified 49 news and information sites that appeared to be almost entirely written by AI tools. Since then, the company has spotted hundreds of additional unreliable, AI-generated websites.
“It’s really a volume game,” Parsons said. “They’re just pumping out hundreds — in some cases, thousands — or articles a day, and it’s an ad revenue game. In some cases, they’re just trying to get a lot of content — make it on to search engines and make some programmatic ad revenue. And in some cases, we’re seeing them spread misinformation and disinformation.”
And the barrier to entry is lowering.
Another NewsGuard study, published in late March, found that OpenAI’s flagship text-generating model, GPT-4, is more likely to spread misinformation when prompted than its predecessor, GPT-3.5. NewsGuard’s test found that GPT-4 was better at elevating false narratives in more convincing ways across a range of formats, including “news articles, Twitter threads, and TV scripts mimicking Russian and Chinese state-run media outlets, health hoax peddlers, and well-known conspiracy theorists.”
So what’s the answer to that dilemma? It’s not immediately clear.
Parsons pointed out that Adobe, which maintains a family of generative AI products called Firefly, implements safeguards, like filters, aimed at preventing misuse. And the Content Authenticity Initiative, which Adobe co-founded in 2019 with the New York Times and Twitter, promotes an industry standard for provenance metadata.
But use of the CAI’s standard is completely voluntary, and just because Adobe is implementing safeguards doesn’t mean others will follow suit — or that those safeguards can’t or won’t be bypassed.
The panelists floated watermarking as another useful measure, albeit not a panacea.
A number of organizations are exploring watermarking techniques for generative media, including DeepMind, which recently proposed a standard, SynthID, to mark AI-generated images in a way that’s imperceptible to the human eye but can be easily spotted by a specialized detector. French startup Imatag, launched in 2020, offers a watermarking tool that it claims isn’t affected by resizing, cropping, editing or compressing images, similar to SynthID. Yet another firm, Steg.AI, employs an AI model to apply watermarks that survive resizing and other edits.
Indeed, pointing to some of the watermarking efforts and technologies on the market today, Brandt expressed optimism that “economic incentives” will encourage the companies building generative AI tools to be more thoughtful about how they deploy these tools — and the ways in which they design them to prevent them from being misused.
“With generative AI companies, their content needs to be trustworthy — otherwise, people won’t use it,” she said. “If it continues to hallucinate, if it continues to propagate misinformation, if it continues to not cite sources — that’s going to be less reliable than whatever generative AI company is making efforts to make sure that their content is reliable.”
I’m not so sure — especially as highly capable, safeguard-free open source generative AI models become widely available. As with all things, I suppose, time will tell.
</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[LLM] Auctoria uses generative AI to create video game models</h2>
                        <p class="card-summary">Polish VR game developer Carbon Studio has launched Auctoria, a platform that uses AI to generate 3D video game assets, including entire game levels, textures, and models, with the aim of providing advanced tools for professionals in the game development industry.</p>
                        <p class="card-text">
                            <a href="https://techcrunch.com/2023/09/20/auctoria-uses-generative-ai-to-create-video-game-models/" class="card-link">https://techcrunch.com/2023/09/20/auctoria-uses-generative-ai-to-create-video-game-models/</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">
Several years ago, Aleksander Caban, the co-founder of Carbon Studio, a Polish VR game developer, observed a major problem in modern game design. He had to create rocks, hills, paths and other basic elements of video game environments manually, which often turned out to be a time-consuming — and laborious — process.
So Caban decided to develop tech to help automate the process.
He teamed up with Michal Bugała, Joanna Zając and two Carbon Studio co-founders, Karolina Koszuta and Błażej Szaflik, to launch Auctoria, a platform that taps AI to generate 3D video game assets from scratch. Based in Gliwice, Poland, Auctoria is one of the participants in the Startup Battlefield 200 at TechCrunch Disrupt 2023.
“We created Auctoria out of a passion for limitless creativity,” Zając told TechCrunch in an email interview. “It was created to support game development professionals in their work, but everyone who wants to create may use it. There’s not a lot of advanced tools for professionals; most of them are focused on hobbyists and amateurs. We want to change that.”
Auctoria uses generative AI tech to create a range of different model types for video games. One of the platform’s features attempts to generate entire 3D game levels, complete with pathways for players to explore (albeit fairly basic ones), while another converts uploaded images and textures of walls, floors and columns into 3D equivalents of that artwork.
Users can also enter text prompts to have Auctoria generate assets, à la DALL-E 2 or Midjourney. Or they can provide a sketch, which the platform will attempt to turn into a usable digital model.
A 3D video game level created with Auctoria. Image Credits: Auctoria
Zając claims that all the AI algorithms powering Auctoria, as well as the data used to train them, were developed in-house.
“Auctoria is based 100% on our content, so we’re not dependent on any other provider,” she said. “It’s an independent tool — Auctoria doesn’t rely on any external engine or use open source solutions.”
Now, Auctoria doesn’t stand alone in the nascent market for AI tools to generate game assets. There’s the 3D model-creating platforms 3DFY and Scenario, as well as startups like Kaedim, Mirage and Hypothetic. Even incumbents such as Nvidia and Autodesk are beginning to dip their toes in the space with apps like Get3D, which converts images to 3D models, and ClipForge, which generates models from text descriptions.
Meta, too, has experimented with tech to generate 3D assets from prompts. So has OpenAI, which last December released Point-E, an AI that synthesizes 3D models with potential applications in 3D printing, game design and animation.
The race to bring new solutions to market isn’t surprising, given the sheer size of the opportunity. According to Proficient Market Insights, the 3D models market could be worth $3.57 billion by 2028.
But Zając claims Auctoria’s relatively long development cycle — it’s been in the R&D stage for roughly two years — has resulted in a more “robust” and “comprehensive” toolset than some rivals offer.
“At present, there’s a lack of AI-based software that enables the creation of complete 3D world models,” Zając said. “Existing solutions typically consist of 3D editors and plugins, but they offer only a fraction of Auctoria’s capabilities. Our team began developing the tool two years ago, allowing us to have a ready-to-use product.”
Of course, as with all generative AI startups, Auctoria will have to contend with the legal challenges currently swirling around AI-generated media. In the U.S. at least, it’s not clear yet to what extent AI-generated works can be copyrighted.
But the Auctoria team — a team of seven employees at present, plus the five co-founders — are leaving those questions unaddressed for now. They’re focusing instead on partnering with game development studios, including Caban’s own Carbon Studio, to pilot the tooling.
Ahead of the Auctoria’s general availability in the coming months, the company hopes to raise as much as $5 million to “speed up the process” of creating back-end cloud services to scale the platform.
“The money would decrease the overall computing time needed to create worlds or 3D models with Auctoria,” Zając said. “Creating infrastructure for a software-as-a-service model is one thing — the other is improving the user experience, for example making it easier to onboard with a simple UI and good customer service and marketing experiences … We’re going to keep our core team small, but by the end of the year, we’re going to hire a few more employees.”
</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[Doc Aug] Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues</h2>
                        <p class="card-summary">This paper investigates the use of large language models for generating responses in information-seeking dialogues, using the MultiDoc2Dial corpus for evaluation, and finds that while the language models may include additional information not present in the relevant documents, they are rated higher than the shared task winning system and human responses.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.11838v1" class="card-link">http://arxiv.org/pdf/2309.11838v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">In this paper, we investigate the use of large language models (LLMs) like
ChatGPT for document-grounded response generation in the context of
information-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus
of task-oriented dialogues in four social service domains previously used in
the DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded
in multiple documents providing relevant information. We generate dialogue
completion responses by prompting a ChatGPT model, using two methods:
Chat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT
model pretraining while LlamaIndex also extracts relevant information from
documents. Observing that document-grounded response generation via LLMs cannot
be adequately assessed by automatic evaluation metrics as they are
significantly more verbose, we perform a human evaluation where annotators rate
the output of the shared task winning system, the two Chat-GPT variants
outputs, and human responses. While both ChatGPT variants are more likely to
include information not present in the relevant segments, possibly including a
presence of hallucinations, they are rated higher than both the shared task
winning system and human responses.</p>
                    </div>
                </div>
                
                <div class="separator"></div>
                
                
                <div class="card mb-4">
                    <div class="card-body">
                        <h2 class="card-title">[AL] Large-scale Pretraining Improves Sample Efficiency of Active Learning based Molecule Virtual Screening</h2>
                        <p class="card-summary">This study examines the performance of pretrained transformer-based language models and graph neural networks in a Bayesian optimization active learning framework for virtual screening of large compound libraries, showing improved accuracy and sample efficiency in drug discovery.</p>
                        <p class="card-text">
                            <a href="http://arxiv.org/pdf/2309.11687v1" class="card-link">http://arxiv.org/pdf/2309.11687v1</a>
                        </p>
                        <label class="read-more-button">
                            <span class="read-more-icon">&#x25BC;</span> <!-- Downward-pointing arrow icon -->
                            <span class="read-more-text">Read More</span>
                            <input type="checkbox" class="read-more-toggle" style="display: none;">
                        </label>
                        <!-- Add the content below the "Read more" button -->
                        <p class="card-content">Virtual screening of large compound libraries to identify potential hit
candidates is one of the earliest steps in drug discovery. As the size of
commercially available compound collections grows exponentially to the scale of
billions, brute-force virtual screening using traditional tools such as docking
becomes infeasible in terms of time and computational resources. Active
learning and Bayesian optimization has recently been proven as effective
methods of narrowing down the search space. An essential component in those
methods is a surrogate machine learning model that is trained with a small
subset of the library to predict the desired properties of compounds. Accurate
model can achieve high sample efficiency by finding the most promising
compounds with only a fraction of the whole library being virtually screened.
In this study, we examined the performance of pretrained transformer-based
language model and graph neural network in Bayesian optimization active
learning framework. The best pretrained models identifies 58.97% of the
top-50000 by docking score after screening only 0.6% of an ultra-large library
containing 99.5 million compounds, improving 8% over previous state-of-the-art
baseline. Through extensive benchmarks, we show that the superior performance
of pretrained models persists in both structure-based and ligand-based drug
discovery. Such model can serve as a boost to the accuracy and sample
efficiency of active learning based molecule virtual screening.</p>
                    </div>
                </div>
                
                
            </div>
        </div>
    </div>
</body>

</html>